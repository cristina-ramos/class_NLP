{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1sDmw5QbJm1lESdOdc-SJ-jCeyy1ag7vq",
      "authorship_tag": "ABX9TyMG0kt3dJQ0hlQosp4gcWBG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristina-ramos/class_NLP/blob/main/Assignments/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpQYDC8afMcw"
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "import numpy as np\n",
        "import math\n",
        "import operator\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import argparse\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8N0Iq3bq63z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad02552-acee-40f3-e121-1e5ae51d3a4b"
      },
      "source": [
        "import nltk\n",
        "import glob\n",
        "import string\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "!wget 'http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz'\n",
        "#not sure if we have to use wget but my colab wouldn't untar my file otherwise\n",
        "!tar -xf review_polarity.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "--2020-11-03 00:28:09--  http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 52.201.128.190\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|52.201.128.190|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3127238 (3.0M) [application/x-gzip]\n",
            "Saving to: ‘review_polarity.tar.gz’\n",
            "\n",
            "review_polarity.tar 100%[===================>]   2.98M  4.75MB/s    in 0.6s    \n",
            "\n",
            "2020-11-03 00:28:10 (4.75 MB/s) - ‘review_polarity.tar.gz’ saved [3127238/3127238]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7ncmwaftB1M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2ecda5b4-3cf5-456f-b131-b0ef7fe4b829"
      },
      "source": [
        "neg_entries = glob.glob('/content/txt_sentoken/neg/*.txt')\n",
        "pos_entries = glob.glob('/content/txt_sentoken/pos/*.txt')\n",
        "#getting number of entries for each folder\n",
        "print(\"The number of positive entries are: \" + str(len(pos_entries)))\n",
        "print(\"The number of negative entries are: \" + str((len(neg_entries))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of positive entries are: 1000\n",
            "The number of negative entries are: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlkn9kO4eYez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5d659f8-6504-482f-ea15-4810d0fb31f3"
      },
      "source": [
        "#Creating and empty list to append both positive and negative datasets:\n",
        "totalwords = []\n",
        "\n",
        "#Cleaning the data and removing punctuation and combining:\n",
        "def merge_texts(directory):\n",
        "  for fn in directory:\n",
        "    if fn in pos_entries:\n",
        "      with open(fn) as f:\n",
        "        message = f.read().split()\n",
        "        for w in message:\n",
        "          if w not in string.punctuation:\n",
        "            totalwords.append(w)\n",
        "    elif fn in neg_entries:\n",
        "      with open(fn) as f:\n",
        "        message = f.read().split()\n",
        "        for w in message:\n",
        "          if w not in string.punctuation:\n",
        "            totalwords.append(w)\n",
        "\n",
        "merge_texts(pos_entries)\n",
        "merge_texts(neg_entries)\n",
        "print(\"The total number of words in the dataset is:\", len(totalwords))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total number of words in the dataset is: 1294462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ2AWbdDfWA8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "91f3103c-c7b3-4241-bfc1-213615b9dd4c"
      },
      "source": [
        "#Getting the top 100 words:\n",
        "freq_count = Counter(totalwords).most_common()\n",
        "top100 = []\n",
        "for word_count in freq_count[0:100]:\n",
        "  word = word_count[0]\n",
        "  top100.append(word)\n",
        "print(\"List of the top 100 words:\", top100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of the top 100 words: ['the', 'a', 'and', 'of', 'to', 'is', 'in', 'that', 'it', 'as', 'with', 'for', 'his', 'this', 'film', 'but', 'he', 'i', 'on', 'are', 'by', 'be', 'an', 'not', 'one', 'movie', 'who', 'at', 'from', 'was', 'have', 'has', 'her', 'you', 'they', 'all', \"it's\", 'so', 'like', 'about', 'out', 'more', 'when', 'which', 'their', 'up', 'or', 'what', 'some', 'just', 'if', 'there', 'she', 'him', 'into', 'even', 'only', 'than', 'no', 'we', 'good', 'most', 'time', 'its', 'can', 'will', 'story', 'been', 'would', 'much', 'also', 'other', 'get', 'character', 'do', 'them', 'very', 'two', 'characters', 'first', 'after', 'see', 'because', 'way', 'well', 'make', 'any', 'does', 'really', 'had', 'too', 'while', 'films', 'how', 'little', 'life', 'where', 'plot', 'off', 'people']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRawBAd5gNXu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a6518c4-3cd5-48c8-b49f-b5e325d8a9a8"
      },
      "source": [
        "#finding # of stopwords in the top 100 using nltk's stopwords corpus\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_count = 0\n",
        "for word in top100:\n",
        "  if word in stop_words:\n",
        "    stop_count += 1\n",
        "print(\"The number of stopwords in the top 100 words are:\", str(stop_count))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of stopwords in the top 100 words are: 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwlvxzSFwiio",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f77831e7-24e7-46cb-c072-73d51740d584"
      },
      "source": [
        "#Most frequent 1000 words:\n",
        "top_1000 = dict(freq_count[0:1000])\n",
        "print(\"Here are the top 1000 words:\", top_1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here are the top 1000 words: {'the': 76178, 'a': 37934, 'and': 35343, 'of': 33970, 'to': 31759, 'is': 25163, 'in': 21591, 'that': 15105, 'it': 12289, 'as': 11338, 'with': 10778, 'for': 9913, 'his': 9566, 'this': 9560, 'film': 8849, 'but': 8576, 'he': 7607, 'i': 7492, 'on': 7241, 'are': 6943, 'by': 6218, 'be': 6082, 'an': 5735, 'not': 5519, 'one': 5514, 'movie': 5429, 'who': 5376, 'at': 4965, 'from': 4948, 'was': 4934, 'have': 4897, 'has': 4714, 'her': 4502, 'you': 4421, 'they': 4276, 'all': 4248, \"it's\": 3696, 'so': 3578, 'like': 3543, 'about': 3518, 'out': 3436, 'more': 3341, 'when': 3250, 'which': 3155, 'their': 3115, 'up': 3096, 'or': 3096, 'what': 3070, 'some': 2980, 'just': 2900, 'if': 2786, 'there': 2755, 'she': 2686, 'him': 2630, 'into': 2616, 'even': 2554, 'only': 2482, 'than': 2438, 'no': 2405, 'we': 2357, 'good': 2313, 'most': 2298, 'time': 2280, 'its': 2265, 'can': 2230, 'will': 2193, 'story': 2110, 'been': 2045, 'would': 2041, 'much': 2022, 'also': 1965, 'other': 1924, 'get': 1920, 'character': 1902, 'do': 1888, 'them': 1876, 'very': 1862, 'two': 1824, 'characters': 1813, 'first': 1766, 'after': 1752, 'see': 1726, 'because': 1682, 'way': 1668, 'well': 1654, 'make': 1590, 'any': 1569, 'does': 1567, 'really': 1556, 'had': 1544, 'too': 1538, 'while': 1537, 'films': 1513, 'how': 1507, 'little': 1487, 'life': 1467, 'where': 1456, 'plot': 1448, 'off': 1420, 'people': 1418, 'then': 1407, 'me': 1401, 'could': 1395, 'my': 1387, 'bad': 1372, 'scene': 1372, 'never': 1360, 'being': 1332, 'these': 1307, 'over': 1298, 'best': 1298, 'new': 1275, 'many': 1267, \"doesn't\": 1266, 'scenes': 1262, 'man': 1255, 'such': 1217, 'were': 1217, 'know': 1207, \"don't\": 1198, 'through': 1180, 'movies': 1180, \"he's\": 1149, 'great': 1138, '--': 1136, 'another': 1111, 'here': 1103, 'love': 1087, 'action': 1073, 'go': 1073, 'us': 1065, 'director': 1054, 'something': 1047, 'end': 1044, 'still': 1037, 'seems': 1032, 'back': 1031, 'made': 1025, 'those': 1013, 'work': 1005, 'makes': 991, \"there's\": 986, 'before': 986, 'however': 986, 'now': 977, 'big': 967, 'few': 964, 'world': 958, 'between': 954, 'every': 945, 'though': 936, 'better': 914, 'enough': 903, 'seen': 902, 'around': 896, 'take': 892, 'both': 892, 'performance': 885, 'why': 878, 'audience': 878, 'going': 871, 'same': 869, \"isn't\": 868, 'down': 868, 'role': 863, 'should': 862, 'gets': 860, 'may': 854, 'real': 849, 'things': 845, 'your': 843, 'think': 842, 'years': 839, 'actually': 834, 'last': 829, 'look': 829, 'funny': 824, 'own': 813, 'almost': 810, 'say': 802, 'nothing': 800, 'thing': 799, 'comedy': 797, 'fact': 795, 'although': 795, 'played': 788, 'right': 780, \"that's\": 779, 'find': 779, 'john': 771, 'since': 768, 'come': 767, 'did': 762, 'script': 761, 'cast': 759, 'plays': 752, 'long': 749, 'young': 738, 'ever': 736, 'comes': 733, 'old': 726, 'part': 703, 'original': 701, 'show': 699, 'without': 695, 'acting': 688, 'actors': 683, 'each': 682, 'again': 681, 'least': 675, 'lot': 674, 'point': 670, 'takes': 668, 'star': 664, 'himself': 656, 'quite': 655, 'during': 654, 'away': 650, 'course': 647, 'goes': 646, \"can't\": 642, 'minutes': 635, 'interesting': 633, 'effects': 629, 'screen': 626, 'three': 625, 'might': 624, \"i'm\": 622, 'rather': 620, 'family': 619, 'guy': 618, 'anything': 617, 'far': 611, 'place': 610, 'day': 609, 'must': 604, 'watch': 601, 'once': 601, 'yet': 600, 'our': 600, 'year': 593, \"film's\": 580, 'seem': 574, \"didn't\": 574, 'always': 573, 'fun': 569, 'trying': 565, 'instead': 565, 'times': 564, 'bit': 564, 'making': 563, 'special': 562, 'give': 559, 'want': 557, 'sense': 553, 'job': 550, 'kind': 549, 'wife': 548, 'having': 547, 'picture': 545, 'set': 544, 'home': 542, 'probably': 537, 'help': 531, 'series': 529, 'along': 529, 'becomes': 526, 'pretty': 523, 'hollywood': 522, 'everything': 521, 'sure': 518, 'together': 515, 'dialogue': 514, 'men': 514, 'woman': 513, 'actor': 512, 'american': 510, 'become': 509, 'gives': 507, 'given': 501, 'hard': 500, 'money': 499, 'black': 494, 'high': 494, 'whole': 494, 'watching': 490, 'wants': 475, 'got': 470, 'music': 467, 'feel': 459, 'perhaps': 458, 'done': 458, 'especially': 454, 'next': 452, 'less': 452, 'death': 451, 'moments': 445, 'play': 445, 'everyone': 444, 'sex': 444, 'looks': 443, 'completely': 439, 'reason': 435, 'looking': 435, 'whose': 434, 'city': 431, 'horror': 431, 'until': 430, 'rest': 430, 'different': 429, 'performances': 429, 'simply': 428, 'james': 421, 'father': 421, 'ending': 421, 'put': 420, 'couple': 420, 'case': 419, 'several': 417, 'mind': 415, \"they're\": 414, 'left': 412, 'anyone': 411, 'evil': 411, 'shows': 409, 'human': 408, 'michael': 407, 'small': 406, 'night': 406, 'itself': 406, 'entire': 406, 'humor': 405, 'getting': 404, 'girl': 402, 'lost': 402, 'turns': 402, \"she's\": 401, 'main': 399, 'found': 397, 'use': 396, 'line': 393, 'begins': 392, 'problem': 392, 'half': 390, 'friends': 388, 'true': 385, 'either': 384, 'soon': 382, 'unfortunately': 382, 'mother': 381, \"i've\": 380, 'later': 380, 'idea': 378, 'stars': 376, 'name': 375, 'comic': 375, 'someone': 375, 'town': 374, 'school': 374, 'wrong': 372, 'thought': 372, 'else': 371, 'tries': 369, 'final': 369, 'friend': 369, 'based': 368, 'group': 367, 'alien': 366, 'against': 366, 'written': 365, 'david': 365, 'house': 364, 'keep': 364, 'used': 364, 'sequence': 364, 'often': 361, '2': 361, 'certainly': 360, 'works': 359, 'second': 359, 'relationship': 359, 'believe': 357, 'called': 356, 'said': 353, 'named': 353, 'dead': 353, 'behind': 351, 'despite': 351, 'playing': 351, 'finally': 350, 'turn': 350, 'head': 348, 'under': 346, 'war': 346, 'maybe': 344, 'doing': 343, 'tell': 341, 'able': 339, 'finds': 337, 'nice': 336, 'seeing': 336, 'perfect': 333, 'past': 333, 'hand': 332, 'book': 330, \"you're\": 329, 'including': 329, 'person': 326, 'shot': 326, 'days': 325, 'mr': 325, 'lives': 324, 'camera': 323, 'boy': 323, 'supposed': 322, 'live': 320, 'lines': 319, 'side': 317, 'moment': 317, 'directed': 316, 'starts': 316, 'need': 316, 'entertaining': 313, 'running': 313, 'full': 312, 'car': 312, 'style': 312, 'fight': 312, 'game': 311, 'run': 311, 'worth': 311, 'summer': 310, 'kids': 309, 'tv': 308, 'worst': 307, 'start': 306, 'upon': 305, 'try': 305, 'kevin': 305, 'matter': 305, 'face': 304, 'care': 302, 'nearly': 302, 'son': 301, 'opening': 301, 'throughout': 300, 'violence': 298, 'example': 298, 'dark': 298, 'exactly': 297, 'hour': 296, 'daughter': 296, 'video': 295, 'early': 295, 's': 294, 'major': 293, 'problems': 293, 'review': 291, 'sequences': 291, 'title': 290, 'beautiful': 290, 'version': 290, 'short': 290, 'production': 289, 'robert': 288, \"wasn't\": 288, 'obvious': 287, 'screenplay': 286, 'classic': 286, 'let': 286, \"who's\": 285, 'already': 285, 'top': 285, 'joe': 285, 'kill': 283, 'direction': 283, 'drama': 283, 'team': 282, 'others': 282, 'order': 282, 'themselves': 281, 'simple': 281, 'children': 281, 'fine': 280, 'roles': 279, 'hit': 278, 'question': 277, 'sort': 276, 'knows': 276, 'act': 276, 'eyes': 276, 'supporting': 275, 'truly': 274, 'white': 272, 'save': 271, 'deep': 271, 'earth': 271, 'sometimes': 269, 'boring': 269, 'known': 266, 'jack': 266, 'beginning': 264, 'women': 264, 'guys': 262, 'coming': 261, 'attempt': 260, 'strong': 259, 'tom': 259, 'jokes': 259, 'killer': 259, 'body': 258, 'happens': 258, \"aren't\": 258, 'four': 258, 'space': 258, 'room': 257, 'york': 256, \"won't\": 256, 'says': 256, 'ends': 256, 'jackie': 255, 'tells': 255, 'novel': 254, 'hope': 253, 'possible': 253, 'peter': 253, 'saw': 252, 'yes': 252, 'quickly': 251, 'scream': 250, 'stupid': 250, 'extremely': 248, 'genre': 248, 'manages': 248, 'lead': 248, 'heart': 247, 'five': 247, 'wonder': 246, 'particularly': 245, 'lee': 245, 'romantic': 244, 'murder': 244, 'stop': 244, 'level': 243, 'appears': 243, 'ship': 243, 'future': 243, 'worse': 242, 'involving': 242, 'career': 242, 'voice': 241, 'involved': 241, 'mostly': 241, 'eventually': 240, 'sets': 240, 'thriller': 240, 'police': 239, 'sound': 238, 'emotional': 238, 'taking': 238, 'attention': 238, \"we're\": 238, 'result': 237, 'material': 237, 'falls': 237, 'dr': 235, 'planet': 235, 'close': 235, 'lack': 235, 'elements': 235, 'hero': 235, 'meet': 234, 'hours': 234, 'child': 234, 'bring': 234, 'piece': 234, 'none': 233, 'fall': 232, 'van': 232, \"what's\": 231, 'brother': 231, 'fiction': 231, 'leads': 231, \"movie's\": 231, 'experience': 230, 'note': 230, 'dog': 230, 'fans': 230, 'living': 229, 'wild': 228, 'enjoy': 227, 'battle': 227, 'de': 227, 'obviously': 226, 'interest': 226, 'paul': 226, 'alone': 226, 'guess': 226, 'usually': 225, 'late': 225, 'among': 225, 'feeling': 225, \"you'll\": 225, 'husband': 224, 'taken': 224, 'laughs': 224, 'power': 223, 'george': 223, 'laugh': 223, 'mean': 222, 'king': 222, 'needs': 221, 'attempts': 221, 'happen': 220, 'within': 220, 'talent': 220, 'chance': 220, 'across': 219, 'number': 219, 'single': 218, 'deal': 218, 'theater': 218, 'hell': 217, 'forced': 217, 'talk': 217, 'whether': 216, 'feels': 216, 'wonderful': 216, 'easy': 216, 'success': 216, 'features': 216, 'killed': 215, 'expect': 215, 'history': 215, 'girls': 215, 'aliens': 214, 'god': 214, 'premise': 214, 'television': 214, 'word': 214, 'leave': 213, 'science': 213, 'impressive': 213, 'words': 213, 'feature': 213, 'except': 212, 'form': 212, 'tale': 212, 'poor': 212, 'chris': 212, 'giving': 212, 'recent': 211, 'seemed': 211, 'call': 210, 'meets': 210, 'score': 210, 'basically': 210, 'disney': 210, 'apparently': 209, 'serious': 209, 'oscar': 208, 'important': 208, 'told': 208, 'surprise': 207, 'parents': 207, 'crew': 207, 'easily': 206, 'parts': 206, 'released': 206, 'stuff': 206, 'somehow': 206, 'entertainment': 205, 'mission': 205, 'robin': 204, 'happy': 204, 'change': 204, 'computer': 204, 'brings': 202, 'art': 201, 'events': 201, 'whom': 201, 'local': 201, 'working': 200, 'release': 200, 'credits': 200, 'remember': 200, 'difficult': 200, 'went': 200, 'ago': 199, 'hilarious': 199, 'crime': 199, 'certain': 198, 'using': 198, 'sequel': 198, 'complete': 197, 'middle': 197, \"wouldn't\": 197, 'am': 197, 'oh': 196, 'william': 196, 'cool': 196, 'due': 195, 'runs': 195, 'turned': 194, 'girlfriend': 194, 'effective': 194, 'ryan': 193, 'ben': 193, 'smith': 193, 'viewer': 192, 'reality': 192, 'presence': 192, 'quality': 192, 'suspense': 192, 'return': 192, 'popular': 191, 'flick': 191, 'uses': 191, 'filmmakers': 191, 'anyway': 190, 'dramatic': 189, 'personal': 189, 'begin': 189, 'surprisingly': 189, '1': 189, 'batman': 189, 'mystery': 188, 'decides': 188, 'figure': 188, 'somewhat': 187, 'ways': 187, 'writing': 187, \"you've\": 187, 'absolutely': 186, 'annoying': 186, 'business': 186, 'blood': 186, 'similar': 186, 'previous': 186, 'die': 185, \"couldn't\": 185, 'came': 185, '3': 185, 'williams': 184, 'light': 184, 'shots': 184, 'gone': 184, 'former': 183, 'project': 183, 'sexual': 183, 'strange': 183, 'latest': 183, 'read': 182, 'towards': 182, 'means': 182, 'excellent': 182, 'nor': 182, 'successful': 182, 'leaves': 181, 'intelligent': 181, 'familiar': 181, 'amazing': 181, 'visual': 181, 'beyond': 180, 'following': 180, 'romance': 180, 'predictable': 180, 'rich': 180, 'clear': 179, 'jim': 179, 'myself': 179, 'cut': 179, 'present': 179, 'leaving': 179, 'questions': 178, 'starring': 178, 'kid': 178, 'talking': 177, 'definitely': 177, 'message': 177, 'add': 176, 'powerful': 176, 'situation': 176, 'herself': 176, 'brilliant': 176, 'party': 176, 'create': 175, 'nature': 175, 'opens': 175, 'type': 175, 'ones': 175, 'stories': 175, 'felt': 175, 'office': 175, 'villain': 175, 'red': 174, 'clever': 174, 'scary': 173, 'smart': 173, 'third': 173, 'cop': 173, 'giant': 173, 'actress': 173, 'cinema': 173, 'bunch': 173, 'usual': 173, 'doubt': 172, 'large': 172, 'company': 172, 'learn': 172, 'age': 172, 'prison': 172, 'move': 171, 'water': 171, 'thinking': 171, 'secret': 171, 'wars': 171, 'solid': 171, 'bill': 171, 'straight': 171, 'rock': 170, 'bob': 170, 'saying': 170, 'follows': 170, 'effect': 169, 'seriously': 169, 'writer': 169, 'potential': 169, \"i'd\": 169, 'america': 168, 'huge': 168, 'plan': 167, 'near': 167, 'unlike': 167, 'brothers': 167, 'million': 166, 'general': 165, 'realize': 165, 'understand': 164, 'took': 164, 'perfectly': 164, 'likely': 164, \"i'll\": 164, 'motion': 164, 'decent': 164, 'martin': 164, 'follow': 164, 'animated': 164, 'sam': 163, 'immediately': 163, 'mark': 163, 'married': 163, 'subject': 163, 'happened': 163, 'enjoyable': 163, 'heard': 162, 'created': 162, 'moving': 162, 'viewers': 161, 'fails': 161, 'filled': 161, 'above': 161, 'stay': 161, 'agent': 161, 'sweet': 160, 'audiences': 160, 'points': 160, 'merely': 160, 'overall': 159, 'exciting': 159, 'country': 159, 'break': 159, 'wanted': 159, 'ultimately': 158, 'neither': 158, 'bruce': 158, 'appear': 158, 'slow': 158, 'force': 157, 'private': 157, 'brought': 157, 'impossible': 157, 'escape': 157, 'mess': 157, 'dream': 157, 'richard': 157, 'trouble': 156, 'jones': 156, 'inside': 155, 'wedding': 155, 'tim': 155, 'favorite': 155, 'murphy': 155, 'trek': 154, 'pay': 154, 'otherwise': 154, 'various': 154, 'fan': 154, 'musical': 154, 'scott': 154, 'keeps': 153, 'liked': 153, 'particular': 153, 'political': 153, 'dumb': 152, 'ten': 152, 'chase': 151, 'steve': 151, 'situations': 151, 'harry': 150, 'spend': 150, 'talented': 150, 'effort': 149, 'society': 149, 'truth': 149, 'bond': 149, 'studio': 149, 'silly': 148, 'earlier': 148, 'members': 148, 'slightly': 148, 'focus': 148, 'element': 148, \"haven't\": 147, 'cannot': 147, 'showing': 147, 'offers': 147, 'purpose': 147, 'biggest': 147, 'drug': 147, 'open': 147, 'soundtrack': 146, 'park': 146, 'memorable': 146, 'frank': 145, 'totally': 145, 'fast': 145, 'cold': 145, 'english': 144, 'view': 144, 'ideas': 144, 'state': 144, 'aspect': 143, 'government': 143, 'waste': 143, 'gun': 143, 'eddie': 143, 'wait': 143, 'box': 143, 'eye': 142, 'entirely': 142, 'l': 142, 'law': 142, 'constantly': 142, 'credit': 142, 'ask': 142, 'actual': 142, 'moves': 141, 'gave': 141, 'british': 141, 'hands': 141, 'terrible': 141, 'ability': 140, 'convincing': 140, 'west': 140, 'e': 139, 'spent': 139, 'ridiculous': 139, 'typical': 139, 'setting': 139, 'female': 139, 'fear': 139, 'fairly': 138, 'animation': 138, 'cinematography': 138, 'carter': 138, 'suddenly': 137, 'expected': 137, 'depth': 137, 'killing': 137, 'rating': 137, '10': 137, 'thinks': 137, 'control': 137, 'lots': 137, 'air': 137, 'atmosphere': 137, 'background': 137, 'r': 136, 'tension': 136, 'army': 136, 'sit': 136, 'complex': 135, 'brief': 135, 'beauty': 135, 'violent': 135, 'mars': 135, 'greatest': 134, 'modern': 134, 'impact': 134}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckRiw_UZB2fp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "926fd09b-b543-4d60-b92e-28af247c0b0c"
      },
      "source": [
        "#removing stopwords now\n",
        "top1000 = FreqDist(totalwords).most_common(1000)\n",
        "in_vocab = [word for word in totalwords if word not in stop_words]\n",
        "print(top1000[:10])\n",
        "print(in_vocab[:10])\n",
        "print(len(in_vocab))\n",
        "#okay now i need the most freq 1000 from vocab\n",
        "fdist = FreqDist(in_vocab)\n",
        "vocab = fdist.most_common(1000)\n",
        "print(len(vocab))\n",
        "vocab[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 76178), ('a', 37934), ('and', 35343), ('of', 33970), ('to', 31759), ('is', 25163), ('in', 21591), ('that', 15105), ('it', 12289), ('as', 11338)]\n",
            "['sam', 'matthew', 'broderick', 'astronomer', 'small', 'american', 'town', 'engaged', 'teacher', 'linda']\n",
            "705486\n",
            "1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('film', 8849),\n",
              " ('one', 5514),\n",
              " ('movie', 5429),\n",
              " ('like', 3543),\n",
              " ('even', 2554),\n",
              " ('good', 2313),\n",
              " ('time', 2280),\n",
              " ('story', 2110),\n",
              " ('would', 2041),\n",
              " ('much', 2022)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NGYbIjxchxK"
      },
      "source": [
        "#total texts in both datasets\n",
        "totaltexts = pos_entries + neg_entries\n",
        "texts = []\n",
        "for fn in totaltexts:\n",
        "  if fn in pos_entries:\n",
        "    cat = 1\n",
        "  else:\n",
        "    cat = 0\n",
        "  with open(fn) as f:\n",
        "    message = f.read().split()\n",
        "    message = [w.rstrip(string.punctuation) for w in message]\n",
        "    message = [w.lower() for w in message]\n",
        "    message = [i for i in message if i]\n",
        "    message = [word for word in message if word not in stop_words]\n",
        "    texts.append((message, cat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qAO0yHzakeV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd524575-b23d-4143-a23c-a5c05ce48e6e"
      },
      "source": [
        "#extracting features from the total words...so if a word is present the ouput should be (word, 1) if the word is not present it shoudl be (word, 0)...is that correct?\n",
        "def get_features(document):\n",
        "  feature = dict()\n",
        "  for key, value in vocab:\n",
        "    if key in document:\n",
        "      feature[key] = 1\n",
        "    else:\n",
        "      feature[key] = 0\n",
        "  return feature\n",
        "doc_features = [(get_features(message), cat) for (message, cat) in texts]\n",
        "len(doc_features)\n",
        "# doc_features\n",
        "#works just a little slow\n",
        "#I know you said with defaultdict you don't need to initialize missing keys to zero however when i used defaultdict(int) and commented out the missing keys parts I got another sample sizeerror...so I'm sticking to this method for now"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95I3t-RIErBM"
      },
      "source": [
        "#splitting the total words into train and test set...is this correct?\n",
        "from sklearn.model_selection import train_test_split\n",
        "traindata, testdata = train_test_split(doc_features, test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "351qDY2Bk1Qv"
      },
      "source": [
        "#getting features\n",
        "train = pd.DataFrame(traindata, columns=['Features', 'Labels'])\n",
        "test = pd.DataFrame(testdata, columns=['Features', 'Labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OUlOXV6ldec"
      },
      "source": [
        "train_feats = train['Features']\n",
        "test_feats = test['Features']\n",
        "\n",
        "train_labels = train['Labels']\n",
        "test_labels = test['Labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B6_vGz4lutO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "572d2ec5-ec9f-473f-fd88-0e4f847c97b3"
      },
      "source": [
        "#vectorizing features\n",
        "vector = DictVectorizer(sparse = False)\n",
        "x_train = vector.fit_transform(train_feats.values)\n",
        "x_test = vector.transform(test_feats)\n",
        "\n",
        "y_train = train_labels.values\n",
        "y_test = test_labels.values\n",
        "len(y_test)\n",
        "#not sure if that's the correct #"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9-ybwPzmc10"
      },
      "source": [
        "#creating sigmoid function:\n",
        "def sigmoid (x, w, b):\n",
        "  x = np.array(x)\n",
        "  w = np.array(w)\n",
        "  p = 1 / (1 + math.exp(-(np.dot(w, x) + b)))\n",
        "  return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvpsFa_gmftn"
      },
      "source": [
        "#log regression model:\n",
        "def log_regression(docs, labels):\n",
        "  w = [0] * 1000\n",
        "  b = 0\n",
        "  L = 0.1\n",
        "  it = 10\n",
        "  pred = []\n",
        "  scores = []\n",
        "  for it in range(it):\n",
        "    for (x,y) in zip(docs, labels):\n",
        "      prob = sigmoid(x, w, b)\n",
        "      d_w = (prob - y)*x\n",
        "      d_b = prob - y\n",
        "      w = w - L * d_w\n",
        "      b = b - L * d_b\n",
        "      threshold = 0.5\n",
        "    for x in x_test:\n",
        "      result = sigmoid(x, w, b)\n",
        "      pred.append(1 if result >= threshold else 0)\n",
        "    report = classification_report(y_test, pred)\n",
        "    scores.append(report)\n",
        "    pred = []\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayn1Cmf7B4WB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "cdf37d6f-f10e-4323-85c6-729ab3e2846f"
      },
      "source": [
        "lr = log_regression(x_train, y_train)\n",
        "lr\n",
        "#yay finally"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['              precision    recall  f1-score   support\\n\\n           0       0.80      0.82      0.81       205\\n           1       0.81      0.78      0.79       195\\n\\n    accuracy                           0.80       400\\n   macro avg       0.80      0.80      0.80       400\\nweighted avg       0.80      0.80      0.80       400\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.77      0.87      0.81       205\\n           1       0.84      0.72      0.78       195\\n\\n    accuracy                           0.80       400\\n   macro avg       0.80      0.80      0.80       400\\nweighted avg       0.80      0.80      0.80       400\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.75      0.90      0.82       205\\n           1       0.87      0.69      0.77       195\\n\\n    accuracy                           0.80       400\\n   macro avg       0.81      0.79      0.79       400\\nweighted avg       0.81      0.80      0.80       400\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.78      0.88      0.83       205\\n           1       0.86      0.74      0.79       195\\n\\n    accuracy                           0.81       400\\n   macro avg       0.82      0.81      0.81       400\\nweighted avg       0.82      0.81      0.81       400\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.78      0.87      0.82       205\\n           1       0.84      0.75      0.79       195\\n\\n    accuracy                           0.81       400\\n   macro avg       0.81      0.81      0.81       400\\nweighted avg       0.81      0.81      0.81       400\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.81      0.86      0.83       205\\n           1       0.84      0.78      0.81       195\\n\\n    accuracy                           0.82       400\\n   macro avg       0.82      0.82      0.82       400\\nweighted avg       0.82      0.82      0.82       400\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.80      0.85      0.82       205\\n           1       0.83      0.77      0.80       195\\n\\n    accuracy                           0.81       400\\n   macro avg       0.81      0.81      0.81       400\\nweighted avg       0.81      0.81      0.81       400\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.80      0.85      0.82       205\\n           1       0.83      0.77      0.80       195\\n\\n    accuracy                           0.81       400\\n   macro avg       0.81      0.81      0.81       400\\nweighted avg       0.81      0.81      0.81       400\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.79      0.85      0.82       205\\n           1       0.83      0.76      0.80       195\\n\\n    accuracy                           0.81       400\\n   macro avg       0.81      0.81      0.81       400\\nweighted avg       0.81      0.81      0.81       400\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.79      0.85      0.82       205\\n           1       0.83      0.76      0.79       195\\n\\n    accuracy                           0.81       400\\n   macro avg       0.81      0.81      0.81       400\\nweighted avg       0.81      0.81      0.81       400\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2oT8CELtOa7"
      },
      "source": [
        "# print(x_train.shape)\n",
        "# print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu81--lohUxS"
      },
      "source": [
        "#general idea of the formula for minibatches:\n",
        "\n",
        "# for i in range(iterations):\n",
        "#   np.random.shuffle(data)\n",
        "##not sure shuffling is actually needed\n",
        "#   for batch in random_minibatches(data, batch_size=32):\n",
        "#     grad = compute_gradient(batch, params)\n",
        "#     params = params - learning_rate * grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPmwc1m7IDM0"
      },
      "source": [
        "def minibatches(x, y, batch):\n",
        "  mini_batches = []\n",
        "  for i in np.arange(0, x.shape[0], batch):\n",
        "    x_mini = x[i:i + batch]\n",
        "    y_mini = y[i:i + batch]\n",
        "    mini_batches.append((x_mini, y_mini))\n",
        "  return mini_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MzI4w1wzONx"
      },
      "source": [
        "mb = minibatches(x_train, y_train, batch=32)\n",
        "mb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5fSXv9EUm5T"
      },
      "source": [
        "minis = minibatches(x_train, y_train, batch=32)\n",
        "minis[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpnZaGEaPCGg"
      },
      "source": [
        "def log_regress(docs, labels):\n",
        "  w = [0] * 1000\n",
        "  b = 0\n",
        "  L = 0.1\n",
        "  it = 10\n",
        "  pred = []\n",
        "  scores = []\n",
        "  for it in range(it):\n",
        "    for (b_x, b_y) in minibatches(docs, labels, batch=32):\n",
        "      b_weights = [0]\n",
        "      b_bias = 0\n",
        "      prob = (b_x.dot(np.array(w) - b))\n",
        "      d_w = (prob - b_y)(b_x)\n",
        "      d_b = (prob - b_y)\n",
        "      b_weights += d_w\n",
        "      b_bias += d_b\n",
        "      avg = b_weights / 1000\n",
        "      w = avg - L * d_w\n",
        "      b = b_bias - L * d_b\n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nXw7giVSd5p"
      },
      "source": [
        "testing = log_regress(x_train, y_train)\n",
        "testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX3M8hpxjdzT"
      },
      "source": [
        ">These are all other (failed) attempts I tried for the minibatch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP6q6Rhwppny"
      },
      "source": [
        "# def compute_cost(x, y, params):\n",
        "#     num_samples = len(x_train)\n",
        "#     cost_sum = 0.0\n",
        "#     for x,y in zip(x_train, y_train):\n",
        "#         y_hat = np.dot(params, np.array([1.0, x]))\n",
        "#         cost_sum += (y_hat - y) ** 2\n",
        "    \n",
        "#     cost = cost_sum / (num_samples * 2.0)\n",
        "    \n",
        "#     return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLfm5c81qHud"
      },
      "source": [
        "# def lin_reg_batch_gradient_descent(x, y, params, alpha, max_iter):\n",
        "#     iteration = 0\n",
        "#     num_samples = len(x_train)\n",
        "#     cost = np.zeros(max_iter)\n",
        "#     params_store = np.zeros([2, max_iter])\n",
        "    \n",
        "#     while iteration < max_iter:\n",
        "#         cost[iteration] = compute_cost(x, y, params)\n",
        "#         params_store[:, iteration] = params\n",
        "        \n",
        "#         print('--------------------------')\n",
        "#         print(f'iteration: {iteration}')\n",
        "#         print(f'cost: {cost[iteration]}')\n",
        "        \n",
        "#         for x,y in zip(x_train, y_train):\n",
        "#             y_hat = np.dot(params, np.array([1.0, x]))\n",
        "#             gradient = np.array([1.0, x]) * (y - y_hat)\n",
        "#             params += alpha * gradient/num_samples\n",
        "            \n",
        "#         iteration += 1\n",
        "    \n",
        "#     return params, cost, params_store\n",
        "\n",
        "# params_0 = np.array([20.0, 80.0])\n",
        "# alpha_batch = 0.1\n",
        "# lin_reg_batch_gradient_descent(x_train, y_train, params=params_0, alpha=alpha_batch, max_iter=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8wyc2X0ck2J"
      },
      "source": [
        "# def next_batch(x, y, batch_size):\n",
        "#   for i in np.arange(0, x.shape[0], batch_size):\n",
        "#     yield (x[i:i + batch_size], y[i:i + batch_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHl-D3CKePru"
      },
      "source": [
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-e\", \"--epochs\", type=float, default=10)\n",
        "# ap.add_argument(\"-a\", \"--alpha\", type=float, default=0.1)\n",
        "# ap.add_argument(\"-b\", \"--batch-size\", type=int, default=32)\n",
        "# args = vars(ap.parse_args())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdaeDGsxypz_"
      },
      "source": [
        "# learn_rates = [0.0]\n",
        "# num_iter = 10\n",
        "# models = {}\n",
        "# for i in learn_rates:\n",
        "#   print(\"Learning rate is: \", i)\n",
        "#   models[i] = [x_train, y_train, x_test, y_test]\n",
        "#   print(\"------------------------------\")\n",
        "# for i in learn_rates:\n",
        "#   plt.plot(np.squeeze(models[i][\"costs\"]), label= str(model[i][\"learn_rate\"]))\n",
        "\n",
        "# plt.ylabel('cost')\n",
        "# plt.xlabel('iterations')\n",
        "# legend = plt.legend(loc='upper center', shadow=True)\n",
        "# frame = legend.get_frame()\n",
        "# frame.set_facecolor('0.90')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}